{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data\n",
    "----------------\n",
    "* `mnist.train` - 55k data points of training data \n",
    "* `mnist.test` - 10k data points of test data\n",
    "* `mnist.validation` - 5k data points of validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing images as vectors\n",
    "------------------------------\n",
    "* `mnist.train.images` - 55000 x 784 matrix, each row is a vector representation of an image. Each element in the vector represents the intensity of each pixel in the image (28 x 28 = 784).\n",
    "* `mnist.train.labels` - 55000 x 10 matrix, each row is a one hot encoding of the digit in the image vector (1-10)\n",
    "\n",
    "<img src=\"mnist_100_digits.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.13333334  0.43529415  0.92549026  0.99607849  0.99607849\n",
      "  0.50588238  0.07058824  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.08235294  0.45098042  0.99607849  0.99215692  0.99215692  0.99215692\n",
      "  0.99215692  0.99215692  0.627451    0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.30588236  0.99215692  0.99607849  0.99215692  0.99215692\n",
      "  0.96078438  0.8588236   0.99215692  0.96862751  0.15294118  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.05490196  0.74117649  0.99215692  0.99607849  0.95686281\n",
      "  0.99215692  0.90588242  0.02745098  0.75294125  0.99215692  0.38823533\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.1254902   0.8588236   0.99215692  0.99215692\n",
      "  0.81960791  0.36470589  0.99215692  0.90588242  0.          0.27450982\n",
      "  0.97647065  0.38823533  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.627451    0.99215692\n",
      "  0.99215692  0.99215692  0.09411766  0.60000002  0.99215692  0.41568631\n",
      "  0.          0.27450982  0.97647065  0.38823533  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01568628\n",
      "  0.91372555  0.99215692  0.99215692  0.76470596  0.          0.69411767\n",
      "  0.80392164  0.02352941  0.          0.5529412   0.97254908  0.20392159\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.13333334  0.74117649  0.99215692  0.99215692  0.80784321  0.0509804   0.\n",
      "  0.69411767  0.60392159  0.          0.          0.78039223  0.87450987\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.21960786  0.99215692  0.99215692  0.99215692  0.36862746\n",
      "  0.          0.          0.6156863   0.23137257  0.          0.\n",
      "  0.78039223  0.51372552  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.03921569  0.74509805  0.99215692  0.99215692\n",
      "  0.84313732  0.03529412  0.          0.          0.05882353  0.          0.\n",
      "  0.19215688  0.77647066  0.51372552  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.52156866  0.99607849  0.99607849\n",
      "  0.99607849  0.57647061  0.          0.          0.          0.          0.\n",
      "  0.          0.65490198  0.99607849  0.27450982  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.52156866  0.99215692\n",
      "  0.99215692  0.98039222  0.28627452  0.          0.          0.          0.\n",
      "  0.          0.17647059  0.89019614  0.90196085  0.0627451   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.52156866\n",
      "  0.99215692  0.99215692  0.78039223  0.          0.          0.          0.\n",
      "  0.          0.          0.6901961   0.99215692  0.29019609  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.52156866  0.99215692  0.99215692  0.29019609  0.          0.          0.\n",
      "  0.          0.          0.37647063  0.99215692  0.97647065  0.19607845\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.6156863   0.99215692  0.99215692  0.27450982  0.          0.\n",
      "  0.          0.          0.          0.67450982  0.97647065  0.41960788\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.95294124  0.99215692  0.99215692  0.96470594\n",
      "  0.13725491  0.          0.          0.          0.09411766  0.88235301\n",
      "  0.90588242  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.38039219  0.99215692  0.98431379\n",
      "  0.96470594  0.99215692  0.9450981   0.63137257  0.23137257  0.10196079\n",
      "  0.80000007  0.99215692  0.3019608   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.74901962\n",
      "  0.99215692  0.50980395  0.13725491  0.7843138   0.81960791  0.56862748\n",
      "  0.79215693  1.          0.99215692  0.75294125  0.01568628  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.71372551  0.92941183  0.09019608  0.          0.          0.          0.\n",
      "  0.56470591  0.99607849  0.99215692  0.28235295  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.82352948  0.71764708  0.          0.          0.          0.\n",
      "  0.13333334  0.92156869  0.99607849  0.57647061  0.02352941  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print mnist.train.images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multilayer Perceptron Algorithm\n",
    "-----------------------------------\n",
    "Our goal is to learn $f(x) = y$, where $x$ is an unseen (i.e. not in our training set) image and $y$ is a digit between 1 and 10. To accomplish this we'll use a type of feedforward artificial neural network called an MLP.\n",
    "\n",
    "An MLP consists of the following components:\n",
    "\n",
    "* Initial weights and biases (random)\n",
    "* Hidden layers\n",
    "   * Activation function (eg. Sigmoid, ReLu)\n",
    "* Output layer = the size of the number of classes to predict from\n",
    "* A cost function/optimizer\n",
    "   * eg. Softmax with cross-entropy, L1/L2\n",
    "   \n",
    "<img src=\"tikz11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing initial weights and biases\n",
    "---------------------------------------\n",
    "\n",
    "Each layers contains:\n",
    "* $W$ = initial random weights (between 0 and 1)\n",
    "* $b$ = initial random biases (between 0 and 1)\n",
    "\n",
    "Initial layer is always the size of our input vector. Output layer is the number of classes we want to predict from. Size of hidden layers is arbitrary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 256 \n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "W = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "b = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the layers and input\n",
    "---------------------------------\n",
    "\n",
    "In each pass of the network, an input example \"propagates\" through each layer. We compute $z_n$ for each node in the layer and apply the activation function $\\delta$. We treat $a_n$ as the input value for the next layer.\n",
    "\n",
    "$$z_1 = Wx + b$$\n",
    "\n",
    "$$a_1 = \\theta(z)$$\n",
    "\n",
    "$$z_2 = Wa_1 + b$$ \n",
    "\n",
    "$$...$$\n",
    "\n",
    "where $x$ is the \"input\" value at each layer.\n",
    "\n",
    "Each layer depends on the last! This is the forward propogation algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "def multilayer_perceptron(_X, _weights, _biases):\n",
    "    #Hidden layer with RELU activation\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) \n",
    "    #Hidden layer with RELU activation\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, _weights['h2']), _biases['b2'])) \n",
    "    return tf.matmul(layer_2, W['out']) + b['out']\n",
    "\n",
    "pred = multilayer_perceptron(x, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define our cost and optimizer\n",
    "-------------------------\n",
    "\n",
    "Recall that our goal is to learn $f(x) = y$. How do we know how good $f(x)$ is? When we train a network, we want some function $Cost(y, y') = \\epsilon$ that tells us how off our predictions are ($y'$ is the predicted value).\n",
    "\n",
    "A few examples:\n",
    "\n",
    "* L2 loss with MSE:\n",
    "$$Cost(y, y') = \\frac{1}{n}\\sum_{i = 1}^{n}(y'_i - y_i)^2$$\n",
    "* Softmax with cross entropy:\n",
    "$$Cost(y, y') = \\sum_{i = 1}^{n} y_i \\ln{(\\frac{1}{y'_i})}$$\n",
    "\n",
    "A few notes on softmax with cross entropy:\n",
    "* \"Average length of communicating an event from one distribution with the optimal code fon another distribution\"\n",
    "* Only compares the one non-zero output value!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Backpropagation\n",
    "---------------\n",
    "General plan of attack:\n",
    "\n",
    "1. Compute the gradient at our output layer (derivative of the cost function w.r.t. activation function):\n",
    "\n",
    "$$\\delta = {Cost}'(y',y)$$\n",
    "\n",
    "2. Update output layer:\n",
    "\n",
    "    $$b_L = \\delta$$\n",
    "    $$W_L = \\delta * a_{L}^T$$\n",
    "  \n",
    "    \n",
    "3. Update layers $\\{L-1, L-2, ... 0\\}$:\n",
    "\n",
    "  $$z_L = W_L + b_L$$\n",
    "\n",
    "    $$\\delta = W_L^T \\delta * \\theta'(z_{L-1})$$\n",
    "    $$b_{L-1} = \\delta$$\n",
    "    $$W_{L-1} = \\delta * a_{L-2}^T$$\n",
    "  \n",
    "    $$\\delta = W_{L-1}^T \\delta * \\theta'(z_{L-2})$$\n",
    "    $$b_{L-2} = \\delta$$\n",
    "    $$W_{L-2} = \\delta * a_{L-3}^T$$\n",
    "    \n",
    "    $$...$$\n",
    "\n",
    "\n",
    "Computes gradient values which are derived from our cost function\n",
    "\n",
    "Intuition: \"Penalize weights that caused error by the amount of error they caused\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network and calculate accuracy\n",
    "----------------------------------------\n",
    "Each step of the loop, we get a \"batch\" of `batch_size` random data points from the training set. Each training step feeds in batch of data to replace the `placeholders` and updates the network (backpropagation) once for every batch. This is stochastic gradient descent!\n",
    "\n",
    "* `correct_prediction` - evaluates to true if the prediction and a test label match up (uses `tf.arg_max` to find index of highest value across a dimension)\n",
    "* `accuracy` - percentage of correct predictions (converts booleans to binary with `tf.cast`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 35.008997646\n",
      "Epoch: 0002 cost= 6.062125091\n",
      "Epoch: 0003 cost= 3.722721570\n",
      "Epoch: 0004 cost= 2.592354636\n",
      "Epoch: 0005 cost= 1.893606476\n",
      "Epoch: 0006 cost= 1.499699955\n",
      "Epoch: 0007 cost= 1.195059822\n",
      "Epoch: 0008 cost= 0.938370555\n",
      "Epoch: 0009 cost= 0.806863983\n",
      "Epoch: 0010 cost= 0.685367677\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9294\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 10\n",
    "display_step = 1\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "    print \"Optimization Finished!\"\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print \"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " mnist.train.next_batch(100)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
